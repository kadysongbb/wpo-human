{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# DR TRPO related files\n",
    "from train_helper import *\n",
    "from value import NNValueFunction\n",
    "from utils import Logger\n",
    "from dr_policy import DRPolicyKL, DRPolicyWass, DRPolicySinkhorn\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as dist\n",
    "import tensorflow.contrib.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Network Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, sess, hidden_size, lr, name):\n",
    "        self.sess = sess\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        self.name = name\n",
    "\n",
    "        self.ob_ac = tf.placeholder(dtype=tf.float32, shape=[None, 2])\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        with tf.variable_scope('discriminator'):\n",
    "            d_h1 = layers.fully_connected(self.ob_ac, self.hidden_size, activation_fn=tf.tanh)\n",
    "            d_h2 = layers.fully_connected(d_h1, self.hidden_size, activation_fn=tf.tanh)\n",
    "            d_out = layers.fully_connected(d_h2, 1, activation_fn=None)\n",
    "\n",
    "        self.reward = - tf.squeeze(tf.log(tf.sigmoid(d_out)))\n",
    "        \n",
    "        expert_out, policy_out = tf.split(d_out, num_or_size_splits=2, axis=0)\n",
    "\n",
    "        self.loss = (tf.losses.sigmoid_cross_entropy(tf.ones_like(policy_out), policy_out)\n",
    "                     + tf.losses.sigmoid_cross_entropy(tf.zeros_like(expert_out), expert_out))\n",
    "        \n",
    "        with tf.name_scope('train_op'):\n",
    "            grads = tf.gradients(self.loss, self.params())\n",
    "            self.grads = list(zip(grads, self.params()))\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).apply_gradients(self.grads)\n",
    "\n",
    "    def params(self):\n",
    "        return tf.global_variables(self.name).copy()\n",
    "\n",
    "    def get_reward(self, expert_ob_ac):\n",
    "        feed_dict = {self.ob_ac: expert_ob_ac}\n",
    "\n",
    "        return self.sess.run(self.reward, feed_dict=feed_dict)\n",
    "\n",
    "    def update(self, all_ob_ac):\n",
    "        feed_dict = {self.ob_ac: all_ob_ac}\n",
    "        self.sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Taxi-v3'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, rl_env, discriminator, lamb):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.action_space = rl_env.action_space\n",
    "        self.observation_space = rl_env.observation_space\n",
    "        self.rl_env = rl_env\n",
    "        self.discriminator = discriminator\n",
    "        self.lamb = lamb\n",
    "    \n",
    "    def step(self, action):\n",
    "        observation, rl_reward, done, info = self.rl_env.step(action)\n",
    "        il_reward = self.discriminator.get_reward(np.asarray([observation, action]).reshape(1,2))\n",
    "        reward = lamb*rl_reward + (1-lamb)*il_reward\n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.rl_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline + GAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kady/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps: 1000 Rewards Sum: -760.7\n",
      "Timesteps: 2000 Rewards Sum: -714.8\n",
      "Timesteps: 3000 Rewards Sum: -654.3\n",
      "Timesteps: 4000 Rewards Sum: -647.3\n",
      "Timesteps: 5000 Rewards Sum: -609.5\n",
      "Timesteps: 6000 Rewards Sum: -541.1\n",
      "Timesteps: 7000 Rewards Sum: -560.3\n",
      "Timesteps: 8000 Rewards Sum: -531.2\n",
      "Timesteps: 9000 Rewards Sum: -478.1\n",
      "Timesteps: 10000 Rewards Sum: -468.2\n",
      "Timesteps: 11000 Rewards Sum: -433.1\n",
      "Timesteps: 12000 Rewards Sum: -414.2\n",
      "Timesteps: 13000 Rewards Sum: -387.2\n",
      "Timesteps: 14000 Rewards Sum: -369.2\n",
      "Timesteps: 15000 Rewards Sum: -316.1\n",
      "Timesteps: 16000 Rewards Sum: -318.8\n",
      "Timesteps: 17000 Rewards Sum: -297.2\n",
      "Timesteps: 18000 Rewards Sum: -269.3\n",
      "Timesteps: 19000 Rewards Sum: -264.8\n",
      "Timesteps: 20000 Rewards Sum: -253.1\n",
      "Timesteps: 21000 Rewards Sum: -243.2\n",
      "Timesteps: 22000 Rewards Sum: -237.8\n",
      "Timesteps: 23000 Rewards Sum: -236.0\n",
      "Timesteps: 24000 Rewards Sum: -220.7\n",
      "Timesteps: 25000 Rewards Sum: -216.2\n",
      "Timesteps: 26000 Rewards Sum: -206.3\n",
      "Timesteps: 27000 Rewards Sum: -211.7\n",
      "Timesteps: 28000 Rewards Sum: -209.9\n",
      "Timesteps: 29000 Rewards Sum: -204.5\n",
      "Timesteps: 30000 Rewards Sum: -207.2\n",
      "Timesteps: 31000 Rewards Sum: -207.2\n",
      "Timesteps: 32000 Rewards Sum: -207.2\n",
      "Timesteps: 33000 Rewards Sum: -203.6\n",
      "Timesteps: 34000 Rewards Sum: -202.7\n",
      "Timesteps: 35000 Rewards Sum: -202.7\n",
      "Timesteps: 36000 Rewards Sum: -202.7\n",
      "Timesteps: 37000 Rewards Sum: -200.9\n",
      "Timesteps: 38000 Rewards Sum: -200.0\n",
      "Timesteps: 39000 Rewards Sum: -200.9\n",
      "Timesteps: 40000 Rewards Sum: -200.0\n",
      "Timesteps: 41000 Rewards Sum: -200.0\n",
      "Timesteps: 42000 Rewards Sum: -200.9\n",
      "Timesteps: 43000 Rewards Sum: -200.9\n",
      "Timesteps: 44000 Rewards Sum: -200.0\n",
      "Timesteps: 45000 Rewards Sum: -200.9\n",
      "Timesteps: 46000 Rewards Sum: -200.0\n",
      "Timesteps: 47000 Rewards Sum: -200.0\n",
      "Timesteps: 48000 Rewards Sum: -202.7\n",
      "Timesteps: 49000 Rewards Sum: -200.9\n",
      "Timesteps: 50000 Rewards Sum: -200.0\n",
      "Timesteps: 51000 Rewards Sum: -200.9\n",
      "Timesteps: 52000 Rewards Sum: -200.9\n",
      "Timesteps: 53000 Rewards Sum: -200.0\n",
      "Timesteps: 54000 Rewards Sum: -201.8\n",
      "Timesteps: 55000 Rewards Sum: -200.0\n",
      "Timesteps: 56000 Rewards Sum: -200.9\n",
      "Timesteps: 57000 Rewards Sum: -200.0\n",
      "Timesteps: 58000 Rewards Sum: -200.0\n",
      "Timesteps: 59000 Rewards Sum: -200.0\n",
      "Timesteps: 60000 Rewards Sum: -200.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines import TRPO\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "device_count={'GPU': 1},\n",
    "intra_op_parallelism_threads=1,\n",
    "allow_soft_placement=True\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "tf.keras.backend.set_session(sess)\n",
    "discriminator = Discriminator(sess, 10, 0.01, 'D')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.reset_default_graph()\n",
    "\n",
    "ts = 0\n",
    "total_ts = 60000\n",
    "batch_ts = 1000\n",
    "lamb = 0.9\n",
    "gamma = 0.9\n",
    "custom_env = CustomEnv(env, discriminator, lamb)\n",
    "data = np.load('expert_traj/expert_taxi.npz')\n",
    "expert_obs = data['obs'].T[0]\n",
    "expert_actions = data['actions'].T[0]\n",
    "expert_ob_ac = np.stack((expert_obs, expert_actions),1)\n",
    "custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "\n",
    "model = TRPO(MlpPolicy, custom_env, verbose=0)\n",
    "\n",
    "while ts < total_ts: \n",
    "    model.learn(total_timesteps = batch_ts)\n",
    "    ts += batch_ts\n",
    "    \n",
    "    # update discriminator \n",
    "    obs = custom_env.reset()[0]\n",
    "    observes, actions, rewards, rew_sum = [],[],[],[]\n",
    "    i = 0\n",
    "    while i < len(expert_ob_ac):\n",
    "        observes.append(obs)\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            obs = custom_env.reset()[0]\n",
    "            rew_sum.append(np.sum(rewards))\n",
    "            rewards = []\n",
    "        i += 1\n",
    "    policy_ob_ac = np.stack((observes, actions),1)\n",
    "    discriminator.update(np.concatenate([expert_ob_ac, policy_ob_ac], axis=0))\n",
    "    custom_env = CustomEnv(env, discriminator, lamb)\n",
    "    custom_env = DummyVecEnv(([lambda: custom_env]))\n",
    "    model.set_env(custom_env)\n",
    "    print('Timesteps: ' + str(ts) + ' Rewards Sum: ' + str(np.mean(rew_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
