{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# DR TRPO related files\n",
    "from train_helper import *\n",
    "from value import NNValueFunction\n",
    "from utils import Logger\n",
    "from dr_policy import DRPolicyKL, DRPolicyWass, DRPolicySinkhorn\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as dist\n",
    "import tensorflow.contrib.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, sess, hidden_size, lr, name):\n",
    "        self.sess = sess\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lr = lr\n",
    "        self.name = name\n",
    "\n",
    "        self.ob_ac = tf.placeholder(dtype=tf.float32, shape=[None, 2])\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        with tf.variable_scope('discriminator'):\n",
    "            d_h1 = layers.fully_connected(self.ob_ac, self.hidden_size, activation_fn=tf.tanh)\n",
    "            d_h2 = layers.fully_connected(d_h1, self.hidden_size, activation_fn=tf.tanh)\n",
    "            d_out = layers.fully_connected(d_h2, 1, activation_fn=None)\n",
    "\n",
    "        self.reward = - tf.squeeze(tf.log(tf.sigmoid(d_out)))\n",
    "        \n",
    "        expert_out, policy_out = tf.split(d_out, num_or_size_splits=2, axis=0)\n",
    "\n",
    "        self.loss = (tf.losses.sigmoid_cross_entropy(tf.ones_like(policy_out), policy_out)\n",
    "                     + tf.losses.sigmoid_cross_entropy(tf.zeros_like(expert_out), expert_out))\n",
    "        \n",
    "        with tf.name_scope('train_op'):\n",
    "            grads = tf.gradients(self.loss, self.params())\n",
    "            self.grads = list(zip(grads, self.params()))\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).apply_gradients(self.grads)\n",
    "\n",
    "    def params(self):\n",
    "        return tf.global_variables(self.name).copy()\n",
    "\n",
    "    def get_reward(self, expert_ob_ac):\n",
    "        feed_dict = {self.ob_ac: expert_ob_ac}\n",
    "\n",
    "        return self.sess.run(self.reward, feed_dict=feed_dict)\n",
    "\n",
    "    def update(self, all_ob_ac):\n",
    "        feed_dict = {self.ob_ac: all_ob_ac}\n",
    "\n",
    "        self.sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete State Space - KL DRPO \n",
    "### 'Taxi-v3', 'Roulette-v0', 'NChain-v0', 'FrozenLake-v0', 'CliffWalking-v0', 'FrozenLake8x8-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_name = 'Taxi-v3'\n",
    "env = gym.make(env_name)\n",
    "sta_num = env.observation_space.n\n",
    "act_num = env.action_space.n\n",
    "policy = DRPolicyKL(sta_num, act_num)\n",
    "val_func = NNValueFunction(1, 10)\n",
    "gamma = 0.9\n",
    "lam = 1\n",
    "total_eps = 1000\n",
    "batch_eps = 60\n",
    "logger = Logger(logname=env_name + '_DR-KL_Batch=' + str(batch_eps), now=datetime.utcnow().strftime(\"%b-%d_%H:%M:%S\"))\n",
    "\n",
    "\n",
    "eps = 0\n",
    "while eps < total_eps:\n",
    "        trajectories = run_policy(env, policy, batch_eps, logger)\n",
    "        eps += len(trajectories)\n",
    "        # add estimated values to episodes\n",
    "        add_value(trajectories, val_func)  \n",
    "        # calculated discounted sum of Rs\n",
    "        add_disc_sum_rew(trajectories, gamma, logger)  \n",
    "        # calculate advantage\n",
    "        add_gae(trajectories, gamma, lam)  \n",
    "        # concatenate all episodes into single NumPy arrays\n",
    "        observes, actions, advantages, disc_sum_rew = build_train_set(trajectories)\n",
    "        log_batch_stats(observes, actions, advantages, disc_sum_rew, eps, logger)\n",
    "        disc_freqs = find_disc_freqs(trajectories, sta_num, gamma)\n",
    "        policy.update(observes, actions, advantages, disc_freqs, env_name, eps)\n",
    "        val_func.fit(observes, disc_sum_rew, logger)\n",
    "        # write logger results to file and stdout\n",
    "        logger.write(display=True) \n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete State Space - Sinkhorn DRPO \n",
    "### 'Taxi-v3', 'Roulette-v0', 'NChain-v0', 'FrozenLake-v0', 'CliffWalking-v0', 'FrozenLake8x8-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kady/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Params -- h1: 10, h2: 7, h3: 5, lr: 0.00378\n",
      "***** Episode 60, Mean Return = -684.2, Mean Discounted Return = -37.2 *****\n",
      "ExplainedVarNew: -4.24e-10\n",
      "ExplainedVarOld: -4.44e-06\n",
      "ValFuncLoss: 537\n",
      "\n",
      "\n",
      "***** Episode 120, Mean Return = -613.5, Mean Discounted Return = -32.3 *****\n",
      "ExplainedVarNew: -4.38e-12\n",
      "ExplainedVarOld: -5.26e-11\n",
      "ValFuncLoss: 138\n",
      "\n",
      "\n",
      "***** Episode 180, Mean Return = -593.1, Mean Discounted Return = -31.7 *****\n",
      "ExplainedVarNew: -4.44e-11\n",
      "ExplainedVarOld: -7.15e-11\n",
      "ValFuncLoss: 92.2\n",
      "\n",
      "\n",
      "***** Episode 240, Mean Return = -516.8, Mean Discounted Return = -27.7 *****\n",
      "ExplainedVarNew: -2.39e-11\n",
      "ExplainedVarOld: -1.86e-11\n",
      "ValFuncLoss: 99.9\n",
      "\n",
      "\n",
      "***** Episode 300, Mean Return = -475.4, Mean Discounted Return = -27.2 *****\n",
      "ExplainedVarNew: -1.79e-11\n",
      "ExplainedVarOld: -8.34e-12\n",
      "ValFuncLoss: 105\n",
      "\n",
      "\n",
      "***** Episode 360, Mean Return = -424.4, Mean Discounted Return = -26.1 *****\n",
      "ExplainedVarNew: -7.65e-10\n",
      "ExplainedVarOld: -4.33e-11\n",
      "ValFuncLoss: 101\n",
      "\n",
      "\n",
      "***** Episode 420, Mean Return = -334.1, Mean Discounted Return = -23.0 *****\n",
      "ExplainedVarNew: -0.00092\n",
      "ExplainedVarOld: -5.35e-10\n",
      "ValFuncLoss: 100\n",
      "\n",
      "\n",
      "***** Episode 480, Mean Return = -349.2, Mean Discounted Return = -24.6 *****\n",
      "ExplainedVarNew: -0.00127\n",
      "ExplainedVarOld: -0.00157\n",
      "ValFuncLoss: 99.8\n",
      "\n",
      "\n",
      "***** Episode 540, Mean Return = -270.3, Mean Discounted Return = -20.6 *****\n",
      "ExplainedVarNew: -0.00909\n",
      "ExplainedVarOld: -0.00316\n",
      "ValFuncLoss: 98.3\n",
      "\n",
      "\n",
      "***** Episode 600, Mean Return = -269.0, Mean Discounted Return = -22.8 *****\n",
      "ExplainedVarNew: -0.00843\n",
      "ExplainedVarOld: -0.00922\n",
      "ValFuncLoss: 105\n",
      "\n",
      "\n",
      "***** Episode 660, Mean Return = -305.2, Mean Discounted Return = -20.4 *****\n",
      "ExplainedVarNew: -0.00885\n",
      "ExplainedVarOld: -0.00748\n",
      "ValFuncLoss: 86.8\n",
      "\n",
      "\n",
      "***** Episode 720, Mean Return = -244.0, Mean Discounted Return = -18.8 *****\n",
      "ExplainedVarNew: -0.00921\n",
      "ExplainedVarOld: -0.0119\n",
      "ValFuncLoss: 92.7\n",
      "\n",
      "\n",
      "***** Episode 780, Mean Return = -217.7, Mean Discounted Return = -18.8 *****\n",
      "ExplainedVarNew: -0.00516\n",
      "ExplainedVarOld: -0.00439\n",
      "ValFuncLoss: 92.2\n",
      "\n",
      "\n",
      "***** Episode 840, Mean Return = -232.7, Mean Discounted Return = -18.2 *****\n",
      "ExplainedVarNew: -0.0128\n",
      "ExplainedVarOld: -0.0114\n",
      "ValFuncLoss: 87.9\n",
      "\n",
      "\n",
      "***** Episode 900, Mean Return = -188.9, Mean Discounted Return = -16.1 *****\n",
      "ExplainedVarNew: -0.00539\n",
      "ExplainedVarOld: -0.00746\n",
      "ValFuncLoss: 81.8\n",
      "\n",
      "\n",
      "***** Episode 960, Mean Return = -164.0, Mean Discounted Return = -14.7 *****\n",
      "ExplainedVarNew: -0.00421\n",
      "ExplainedVarOld: -0.00461\n",
      "ValFuncLoss: 91.6\n",
      "\n",
      "\n",
      "***** Episode 1020, Mean Return = -173.6, Mean Discounted Return = -14.8 *****\n",
      "ExplainedVarNew: -0.00796\n",
      "ExplainedVarOld: -0.00444\n",
      "ValFuncLoss: 85.2\n",
      "\n",
      "\n",
      "***** Episode 1080, Mean Return = -129.3, Mean Discounted Return = -14.8 *****\n",
      "ExplainedVarNew: -0.0103\n",
      "ExplainedVarOld: -0.0123\n",
      "ValFuncLoss: 92.2\n",
      "\n",
      "\n",
      "***** Episode 1140, Mean Return = -139.7, Mean Discounted Return = -12.5 *****\n",
      "ExplainedVarNew: -0.0155\n",
      "ExplainedVarOld: -0.0194\n",
      "ValFuncLoss: 89.4\n",
      "\n",
      "\n",
      "***** Episode 1200, Mean Return = -148.9, Mean Discounted Return = -16.0 *****\n",
      "ExplainedVarNew: -0.00449\n",
      "ExplainedVarOld: -0.00439\n",
      "ValFuncLoss: 81.3\n",
      "\n",
      "\n",
      "***** Episode 1260, Mean Return = -162.3, Mean Discounted Return = -15.5 *****\n",
      "ExplainedVarNew: -0.00432\n",
      "ExplainedVarOld: -0.00419\n",
      "ValFuncLoss: 98.2\n",
      "\n",
      "\n",
      "***** Episode 1320, Mean Return = -151.0, Mean Discounted Return = -15.6 *****\n",
      "ExplainedVarNew: -0.00227\n",
      "ExplainedVarOld: -0.00289\n",
      "ValFuncLoss: 91.9\n",
      "\n",
      "\n",
      "***** Episode 1380, Mean Return = -124.1, Mean Discounted Return = -14.3 *****\n",
      "ExplainedVarNew: -0.00607\n",
      "ExplainedVarOld: -0.00803\n",
      "ValFuncLoss: 103\n",
      "\n",
      "\n",
      "***** Episode 1440, Mean Return = -114.9, Mean Discounted Return = -15.6 *****\n",
      "ExplainedVarNew: -0.00548\n",
      "ExplainedVarOld: -0.0074\n",
      "ValFuncLoss: 91.5\n",
      "\n",
      "\n",
      "***** Episode 1500, Mean Return = -116.0, Mean Discounted Return = -15.2 *****\n",
      "ExplainedVarNew: -0.013\n",
      "ExplainedVarOld: -0.00628\n",
      "ValFuncLoss: 107\n",
      "\n",
      "\n",
      "***** Episode 1560, Mean Return = -126.0, Mean Discounted Return = -14.8 *****\n",
      "ExplainedVarNew: -0.0173\n",
      "ExplainedVarOld: -0.0182\n",
      "ValFuncLoss: 109\n",
      "\n",
      "\n",
      "***** Episode 1620, Mean Return = -122.0, Mean Discounted Return = -14.3 *****\n",
      "ExplainedVarNew: -0.0105\n",
      "ExplainedVarOld: -0.00926\n",
      "ValFuncLoss: 107\n",
      "\n",
      "\n",
      "***** Episode 1680, Mean Return = -94.8, Mean Discounted Return = -13.2 *****\n",
      "ExplainedVarNew: -0.0102\n",
      "ExplainedVarOld: -0.0113\n",
      "ValFuncLoss: 101\n",
      "\n",
      "\n",
      "***** Episode 1740, Mean Return = -94.9, Mean Discounted Return = -14.5 *****\n",
      "ExplainedVarNew: -0.00477\n",
      "ExplainedVarOld: -0.0047\n",
      "ValFuncLoss: 113\n",
      "\n",
      "\n",
      "***** Episode 1800, Mean Return = -115.2, Mean Discounted Return = -15.5 *****\n",
      "ExplainedVarNew: -0.0149\n",
      "ExplainedVarOld: -0.0159\n",
      "ValFuncLoss: 112\n",
      "\n",
      "\n",
      "***** Episode 1860, Mean Return = -71.4, Mean Discounted Return = -13.4 *****\n",
      "ExplainedVarNew: -0.0139\n",
      "ExplainedVarOld: -0.0176\n",
      "ValFuncLoss: 137\n",
      "\n",
      "\n",
      "***** Episode 1920, Mean Return = -76.3, Mean Discounted Return = -12.8 *****\n",
      "ExplainedVarNew: -0.00525\n",
      "ExplainedVarOld: -0.00537\n",
      "ValFuncLoss: 140\n",
      "\n",
      "\n",
      "***** Episode 1980, Mean Return = -138.0, Mean Discounted Return = -14.1 *****\n",
      "ExplainedVarNew: -0.00745\n",
      "ExplainedVarOld: -0.00442\n",
      "ValFuncLoss: 90.4\n",
      "\n",
      "\n",
      "***** Episode 2040, Mean Return = -67.4, Mean Discounted Return = -14.9 *****\n",
      "ExplainedVarNew: -0.00966\n",
      "ExplainedVarOld: -0.00945\n",
      "ValFuncLoss: 141\n",
      "\n",
      "\n",
      "***** Episode 2100, Mean Return = -89.9, Mean Discounted Return = -14.2 *****\n",
      "ExplainedVarNew: -0.0103\n",
      "ExplainedVarOld: -0.0101\n",
      "ValFuncLoss: 124\n",
      "\n",
      "\n",
      "***** Episode 2160, Mean Return = -68.3, Mean Discounted Return = -13.5 *****\n",
      "ExplainedVarNew: -0.013\n",
      "ExplainedVarOld: -0.0108\n",
      "ValFuncLoss: 151\n",
      "\n",
      "\n",
      "***** Episode 2220, Mean Return = -92.2, Mean Discounted Return = -12.6 *****\n",
      "ExplainedVarNew: -0.0241\n",
      "ExplainedVarOld: -0.0117\n",
      "ValFuncLoss: 148\n",
      "\n",
      "\n",
      "***** Episode 2280, Mean Return = -96.1, Mean Discounted Return = -14.7 *****\n",
      "ExplainedVarNew: -0.0254\n",
      "ExplainedVarOld: -0.0238\n",
      "ValFuncLoss: 132\n",
      "\n",
      "\n",
      "***** Episode 2340, Mean Return = -102.2, Mean Discounted Return = -16.0 *****\n",
      "ExplainedVarNew: -0.0222\n",
      "ExplainedVarOld: -0.0203\n",
      "ValFuncLoss: 134\n",
      "\n",
      "\n",
      "***** Episode 2400, Mean Return = -71.6, Mean Discounted Return = -13.7 *****\n",
      "ExplainedVarNew: -0.0244\n",
      "ExplainedVarOld: -0.0331\n",
      "ValFuncLoss: 131\n",
      "\n",
      "\n",
      "***** Episode 2460, Mean Return = -90.0, Mean Discounted Return = -14.5 *****\n",
      "ExplainedVarNew: -0.00735\n",
      "ExplainedVarOld: -0.0179\n",
      "ValFuncLoss: 158\n",
      "\n",
      "\n",
      "***** Episode 2520, Mean Return = -66.4, Mean Discounted Return = -14.1 *****\n",
      "ExplainedVarNew: -0.00815\n",
      "ExplainedVarOld: -0.0087\n",
      "ValFuncLoss: 141\n",
      "\n",
      "\n",
      "***** Episode 2580, Mean Return = -101.5, Mean Discounted Return = -14.2 *****\n",
      "ExplainedVarNew: -0.00804\n",
      "ExplainedVarOld: -0.0108\n",
      "ValFuncLoss: 133\n",
      "\n",
      "\n",
      "***** Episode 2640, Mean Return = -74.6, Mean Discounted Return = -14.7 *****\n",
      "ExplainedVarNew: -0.0101\n",
      "ExplainedVarOld: -0.00914\n",
      "ValFuncLoss: 154\n",
      "\n",
      "\n",
      "***** Episode 2700, Mean Return = -88.2, Mean Discounted Return = -14.2 *****\n",
      "ExplainedVarNew: -0.0133\n",
      "ExplainedVarOld: -0.0114\n",
      "ValFuncLoss: 150\n",
      "\n",
      "\n",
      "***** Episode 2760, Mean Return = -44.5, Mean Discounted Return = -13.0 *****\n",
      "ExplainedVarNew: -0.0192\n",
      "ExplainedVarOld: -0.0199\n",
      "ValFuncLoss: 183\n",
      "\n",
      "\n",
      "***** Episode 2820, Mean Return = -80.3, Mean Discounted Return = -17.3 *****\n",
      "ExplainedVarNew: -0.0216\n",
      "ExplainedVarOld: -0.00971\n",
      "ValFuncLoss: 167\n",
      "\n",
      "\n",
      "***** Episode 2880, Mean Return = -74.5, Mean Discounted Return = -13.6 *****\n",
      "ExplainedVarNew: -0.00943\n",
      "ExplainedVarOld: -0.0127\n",
      "ValFuncLoss: 154\n",
      "\n",
      "\n",
      "***** Episode 2940, Mean Return = -72.2, Mean Discounted Return = -14.6 *****\n",
      "ExplainedVarNew: -0.00987\n",
      "ExplainedVarOld: -0.0108\n",
      "ValFuncLoss: 162\n",
      "\n",
      "\n",
      "***** Episode 3000, Mean Return = -71.8, Mean Discounted Return = -14.9 *****\n",
      "ExplainedVarNew: -0.0083\n",
      "ExplainedVarOld: -0.00672\n",
      "ValFuncLoss: 164\n",
      "\n",
      "\n",
      "***** Episode 3060, Mean Return = -66.2, Mean Discounted Return = -13.6 *****\n",
      "ExplainedVarNew: -0.00766\n",
      "ExplainedVarOld: -0.0105\n",
      "ValFuncLoss: 167\n",
      "\n",
      "\n",
      "***** Episode 3120, Mean Return = -77.1, Mean Discounted Return = -17.0 *****\n",
      "ExplainedVarNew: -0.00795\n",
      "ExplainedVarOld: -0.00835\n",
      "ValFuncLoss: 169\n",
      "\n",
      "\n",
      "***** Episode 3180, Mean Return = -77.3, Mean Discounted Return = -16.9 *****\n",
      "ExplainedVarNew: -0.00986\n",
      "ExplainedVarOld: -0.00691\n",
      "ValFuncLoss: 201\n",
      "\n",
      "\n",
      "***** Episode 3240, Mean Return = -79.0, Mean Discounted Return = -16.5 *****\n",
      "ExplainedVarNew: -0.0124\n",
      "ExplainedVarOld: -0.0118\n",
      "ValFuncLoss: 182\n",
      "\n",
      "\n",
      "***** Episode 3300, Mean Return = -64.1, Mean Discounted Return = -14.4 *****\n",
      "ExplainedVarNew: -0.0112\n",
      "ExplainedVarOld: -0.0152\n",
      "ValFuncLoss: 164\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Episode 3360, Mean Return = -88.2, Mean Discounted Return = -16.5 *****\n",
      "ExplainedVarNew: -0.00771\n",
      "ExplainedVarOld: -0.00834\n",
      "ValFuncLoss: 180\n",
      "\n",
      "\n",
      "***** Episode 3420, Mean Return = -43.9, Mean Discounted Return = -11.6 *****\n",
      "ExplainedVarNew: -0.00606\n",
      "ExplainedVarOld: -0.00571\n",
      "ValFuncLoss: 191\n",
      "\n",
      "\n",
      "***** Episode 3480, Mean Return = -58.7, Mean Discounted Return = -15.2 *****\n",
      "ExplainedVarNew: -0.0151\n",
      "ExplainedVarOld: -0.00853\n",
      "ValFuncLoss: 175\n",
      "\n",
      "\n",
      "***** Episode 3540, Mean Return = -60.6, Mean Discounted Return = -14.7 *****\n",
      "ExplainedVarNew: -0.0107\n",
      "ExplainedVarOld: -0.00726\n",
      "ValFuncLoss: 189\n",
      "\n",
      "\n",
      "***** Episode 3600, Mean Return = -73.0, Mean Discounted Return = -14.2 *****\n",
      "ExplainedVarNew: -0.01\n",
      "ExplainedVarOld: -0.01\n",
      "ValFuncLoss: 206\n",
      "\n",
      "\n",
      "***** Episode 3660, Mean Return = -90.5, Mean Discounted Return = -17.3 *****\n",
      "ExplainedVarNew: -0.0187\n",
      "ExplainedVarOld: -0.0209\n",
      "ValFuncLoss: 191\n",
      "\n",
      "\n",
      "***** Episode 3720, Mean Return = -87.5, Mean Discounted Return = -15.0 *****\n",
      "ExplainedVarNew: -0.00737\n",
      "ExplainedVarOld: -0.00986\n",
      "ValFuncLoss: 167\n",
      "\n",
      "\n",
      "***** Episode 3780, Mean Return = -60.5, Mean Discounted Return = -15.9 *****\n",
      "ExplainedVarNew: -0.0092\n",
      "ExplainedVarOld: -0.00877\n",
      "ValFuncLoss: 210\n",
      "\n",
      "\n",
      "***** Episode 3840, Mean Return = -65.2, Mean Discounted Return = -14.7 *****\n",
      "ExplainedVarNew: -0.0137\n",
      "ExplainedVarOld: -0.00632\n",
      "ValFuncLoss: 199\n",
      "\n",
      "\n",
      "***** Episode 3900, Mean Return = -65.0, Mean Discounted Return = -14.7 *****\n",
      "ExplainedVarNew: -0.0111\n",
      "ExplainedVarOld: -0.00773\n",
      "ValFuncLoss: 178\n",
      "\n",
      "\n",
      "***** Episode 3960, Mean Return = -65.5, Mean Discounted Return = -15.2 *****\n",
      "ExplainedVarNew: -0.0157\n",
      "ExplainedVarOld: -0.0131\n",
      "ValFuncLoss: 220\n",
      "\n",
      "\n",
      "***** Episode 4020, Mean Return = -57.6, Mean Discounted Return = -15.0 *****\n",
      "ExplainedVarNew: -0.0139\n",
      "ExplainedVarOld: -0.0217\n",
      "ValFuncLoss: 175\n",
      "\n",
      "\n",
      "***** Episode 4080, Mean Return = -65.3, Mean Discounted Return = -18.4 *****\n",
      "ExplainedVarNew: -0.0143\n",
      "ExplainedVarOld: -0.0132\n",
      "ValFuncLoss: 207\n",
      "\n",
      "\n",
      "***** Episode 4140, Mean Return = -57.3, Mean Discounted Return = -17.2 *****\n",
      "ExplainedVarNew: -0.0172\n",
      "ExplainedVarOld: -0.0217\n",
      "ValFuncLoss: 193\n",
      "\n",
      "\n",
      "***** Episode 4200, Mean Return = -98.1, Mean Discounted Return = -16.7 *****\n",
      "ExplainedVarNew: -0.00863\n",
      "ExplainedVarOld: -0.0138\n",
      "ValFuncLoss: 189\n",
      "\n",
      "\n",
      "***** Episode 4260, Mean Return = -112.9, Mean Discounted Return = -13.9 *****\n",
      "ExplainedVarNew: -0.0108\n",
      "ExplainedVarOld: -0.0122\n",
      "ValFuncLoss: 173\n",
      "\n",
      "\n",
      "***** Episode 4320, Mean Return = -49.3, Mean Discounted Return = -15.5 *****\n",
      "ExplainedVarNew: -0.017\n",
      "ExplainedVarOld: -0.00791\n",
      "ValFuncLoss: 250\n",
      "\n",
      "\n",
      "***** Episode 4380, Mean Return = -69.9, Mean Discounted Return = -16.4 *****\n",
      "ExplainedVarNew: -0.0238\n",
      "ExplainedVarOld: -0.0221\n",
      "ValFuncLoss: 220\n",
      "\n",
      "\n",
      "***** Episode 4440, Mean Return = -96.0, Mean Discounted Return = -17.7 *****\n",
      "ExplainedVarNew: -0.0147\n",
      "ExplainedVarOld: -0.0176\n",
      "ValFuncLoss: 226\n",
      "\n",
      "\n",
      "***** Episode 4500, Mean Return = -62.7, Mean Discounted Return = -14.3 *****\n",
      "ExplainedVarNew: -0.00794\n",
      "ExplainedVarOld: -0.0103\n",
      "ValFuncLoss: 241\n",
      "\n",
      "\n",
      "***** Episode 4560, Mean Return = -51.4, Mean Discounted Return = -14.3 *****\n",
      "ExplainedVarNew: -0.0118\n",
      "ExplainedVarOld: -0.015\n",
      "ValFuncLoss: 200\n",
      "\n",
      "\n",
      "***** Episode 4620, Mean Return = -91.1, Mean Discounted Return = -17.6 *****\n",
      "ExplainedVarNew: -0.0122\n",
      "ExplainedVarOld: -0.00729\n",
      "ValFuncLoss: 206\n",
      "\n",
      "\n",
      "***** Episode 4680, Mean Return = -71.2, Mean Discounted Return = -16.9 *****\n",
      "ExplainedVarNew: -0.0101\n",
      "ExplainedVarOld: -0.00865\n",
      "ValFuncLoss: 212\n",
      "\n",
      "\n",
      "***** Episode 4740, Mean Return = -45.9, Mean Discounted Return = -15.4 *****\n",
      "ExplainedVarNew: -0.0148\n",
      "ExplainedVarOld: -0.0125\n",
      "ValFuncLoss: 224\n",
      "\n",
      "\n",
      "***** Episode 4800, Mean Return = -55.5, Mean Discounted Return = -15.8 *****\n",
      "ExplainedVarNew: -0.0147\n",
      "ExplainedVarOld: -0.0177\n",
      "ValFuncLoss: 225\n",
      "\n",
      "\n",
      "***** Episode 4860, Mean Return = -67.5, Mean Discounted Return = -17.0 *****\n",
      "ExplainedVarNew: -0.0203\n",
      "ExplainedVarOld: -0.0146\n",
      "ValFuncLoss: 241\n",
      "\n",
      "\n",
      "***** Episode 4920, Mean Return = -52.3, Mean Discounted Return = -14.2 *****\n",
      "ExplainedVarNew: -0.0212\n",
      "ExplainedVarOld: -0.0232\n",
      "ValFuncLoss: 240\n",
      "\n",
      "\n",
      "***** Episode 4980, Mean Return = -61.6, Mean Discounted Return = -15.6 *****\n",
      "ExplainedVarNew: -0.0135\n",
      "ExplainedVarOld: -0.0157\n",
      "ValFuncLoss: 234\n",
      "\n",
      "\n",
      "***** Episode 5040, Mean Return = -85.1, Mean Discounted Return = -17.3 *****\n",
      "ExplainedVarNew: -0.0297\n",
      "ExplainedVarOld: -0.0197\n",
      "ValFuncLoss: 228\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(\n",
    "device_count={'GPU': 1},\n",
    "intra_op_parallelism_threads=1,\n",
    "allow_soft_placement=True\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "tf.keras.backend.set_session(sess)\n",
    "discriminator = Discriminator(sess, 10, 0.01, 'D')\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.reset_default_graph()\n",
    "\n",
    "env_name = 'Taxi-v3'\n",
    "env = gym.make(env_name)\n",
    "sta_num = env.observation_space.n\n",
    "act_num = env.action_space.n\n",
    "policy = DRPolicySinkhorn(sta_num, act_num)\n",
    "val_func = NNValueFunction(1, 10)\n",
    "gamma = 0.9\n",
    "lam = 1\n",
    "total_eps = 5000\n",
    "batch_eps = 60\n",
    "logger = Logger(logname=env_name + '_DR-Sinkhorn_Batch=' + str(batch_eps), now=datetime.utcnow().strftime(\"%b-%d_%H:%M:%S\"))\n",
    "\n",
    "\n",
    "eps = 0\n",
    "while eps < total_eps:\n",
    "        # weight for RL\n",
    "        lamb = 0.9\n",
    "        trajectories = run_policy(env, policy, batch_eps, discriminator, lamb, logger)\n",
    "        eps += len(trajectories)\n",
    "        # add estimated values to episodes\n",
    "        add_value(trajectories, val_func)  \n",
    "        # calculated discounted sum of Rs\n",
    "        add_disc_sum_rew(trajectories, gamma, logger)\n",
    "        # calculate advantage\n",
    "        add_gae(trajectories, gamma, lam)\n",
    "        # concatenate all episodes into single NumPy arrays\n",
    "        observes, actions, advantages, disc_sum_rew = build_train_set(trajectories)\n",
    "        disc_freqs = find_disc_freqs(trajectories, sta_num, gamma)\n",
    "        log_batch_stats(observes, actions, advantages, disc_sum_rew, eps, logger)\n",
    "        policy.update(observes, actions, advantages, disc_freqs, env_name, eps)\n",
    "        val_func.fit(observes, disc_sum_rew, logger)\n",
    "        # write logger results to file and stdout\n",
    "        logger.write(display=True)\n",
    "        \n",
    "        policy_ob_ac = np.stack((observes, actions),1)\n",
    "        expert_ob_ac = policy_ob_ac\n",
    "        discriminator.update(np.concatenate([expert_ob_ac, policy_ob_ac], axis=0))\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete State Space - Wasserstein DRPO \n",
    "### 'Taxi-v3', 'Roulette-v0', 'NChain-v0', 'FrozenLake-v0', 'CliffWalking-v0', 'FrozenLake8x8-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Taxi-v3'\n",
    "env = gym.make(env_name)\n",
    "sta_num = env.observation_space.n\n",
    "act_num = env.action_space.n\n",
    "policy = DRPolicyWass(sta_num, act_num)\n",
    "val_func = NNValueFunction(1, 10)\n",
    "gamma = 0.9\n",
    "lam = 1\n",
    "total_eps = 5000\n",
    "batch_eps = 60\n",
    "logger = Logger(logname=env_name + '_DR-Wass_Batch=' + str(batch_eps), now=datetime.utcnow().strftime(\"%b-%d_%H:%M:%S\"))\n",
    "\n",
    "\n",
    "eps = 0\n",
    "while eps < total_eps:\n",
    "        trajectories = run_policy(env, policy, batch_eps, logger)\n",
    "        eps += len(trajectories)\n",
    "        # add estimated values to episodes\n",
    "        add_value(trajectories, val_func)  \n",
    "        # calculated discounted sum of Rs\n",
    "        add_disc_sum_rew(trajectories, gamma, logger)  \n",
    "        # calculate advantage\n",
    "        add_gae(trajectories, gamma, lam)  \n",
    "        # concatenate all episodes into single NumPy arrays\n",
    "        observes, actions, advantages, disc_sum_rew = build_train_set(trajectories)\n",
    "        disc_freqs = find_disc_freqs(trajectories, sta_num, gamma)\n",
    "        log_batch_stats(observes, actions, advantages, disc_sum_rew, eps, logger)\n",
    "        policy.update(observes, actions, advantages, disc_freqs, env_name, eps)\n",
    "        val_func.fit(observes, disc_sum_rew, logger)\n",
    "        # write logger results to file and stdout\n",
    "        logger.write(display=True) \n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0\n",
    "total_success_dropoff = 0\n",
    "total_illegal_action = 0\n",
    "for i in range(1000):\n",
    "    illegal_action, success_dropoff, eps_length = episode_stats(env, policy)\n",
    "    total_illegal_action += illegal_action\n",
    "    total_success_dropoff += success_dropoff\n",
    "    total_length += eps_length\n",
    "    print('------------------------')\n",
    "print(total_illegal_action/1000)\n",
    "print(total_success_dropoff/1000)\n",
    "print(total_length/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "wasserstein_distance([0, 1], [1, 0], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
